<!DOCTYPE HTML>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Hengkai Guo</title>
    <meta name="author" content="Hengkai Guo">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr style="padding:0px">
                <td style="padding:0px">
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr style="padding:0px">
                                <td style="padding:2.5%;width:63%;vertical-align:middle">
                                    <p class="name" style="text-align: center;">
                                        Hengkai Guo
                                    </p>
                                    <p>I currently lead a team at <a href="https://www.bytedance.com/">ByteDance</a>, focusing on the development of advanced 3D vision and augmented reality (AR) technologies. In my role, I have sprearheaded 3D vision projects, aimed at enhancing AR interaction, 3D editing, and content creation. Prior to joining ByteDance, I completed both my bachelor's and master's degrees from <a href="https://www.tsinghua.edu.cn/">Tsinghua University</a>. Additionally, I have enriched my experience with internships at <a href="https://about.google/">Google</a>, <a href="https://www.hulu.com/">Hulu</a>, <a href="https://www.sensetime.com/">Sensetime</a>, and <a href="https://www.microsoft.com/">Microsoft</a>.
                                    </p>
                                    <p style="text-align:center">
                                        <a href="mailto:guohengkaighk@gmail.com">Email</a> &nbsp;/&nbsp;
                                        <a href="https://scholar.google.com/citations?user=6_cOe58AAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                                        <a href="https://github.com/guohengkai/">Github</a>
                                    </p>
                                </td>
                                <td style="padding:2.5%;width:17%;max-width:17%">
                                    <a href="images/ghk.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 20%;" alt="profile photo" src="images/ghk.jpg" class="hoverZoomLink"></a>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:16px;width:100%;vertical-align:middle">
                                    <h2>Research</h2>
                                    <p>
                                        My primary interest lies in 3D vision and its practical applications, encompassing areas including 3D localization, perception, reconstruction, understanding, and generation.
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:16px;width:30%;vertical-align:middle">
                                    <img style="width:100%" src="images/vda.gif" alt="dise">
                                </td>
                                <td style="padding:8px;width:70%;vertical-align:middle">
                                    <a href="https://videodepthanything.github.io/">
                                        <span class="papertitle">Video Depth Anything: Consistent Depth Estimation for Super-Long Videos</span>
                                    </a>
                                    <br>
                                    Sili Chen,
                                    <strong>Hengkai Guo</strong>,
                                    Shengnan Zhu,
                                    Feihu Zhang,
                                    <a href="http://speedinghzl.github.io/">Zilong Huang</a>,
                                    <a href="https://scholar.google.com.sg/citations?user=Q8iay0gAAAAJ&hl=en">Jiashi Feng</a>,
                                    <a href="https://bingykang.github.io/">Bingyi Kang</a>
                                    <br>
                                    <em>arXiv</em>, 2025
                                    <br>
                                    <a href="https://videodepthanything.github.io/">project page</a>
                                    /
                                    <a href="https://arxiv.org/abs/2501.12375">arXiv</a>
                                    <p></p>
                                    <p>
                                        An accurate, consistent, efficient, and generalizable video depth estimator, capable of supporting videos of any length.
                                    </p>
                                </td>
                            </tr>
                            <tr>
                                <td style="padding:4px;width:30%;vertical-align:middle">
                                    <img style="width:100%" src="images/monoplane.png" alt="dise">
                                </td>
                                <td style="padding:8px;width:70%;vertical-align:middle">
                                    <a href="https://github.com/thuzhaowang/MonoPlane">
                                        <span class="papertitle">MonoPlane: Exploiting Monocular Geometric Cues for Generalizable 3D Plane Reconstruction
                                        </span>
                                    </a>
                                    <br>
                                    <a href="https://thuzhaowang.github.io/">Wang Zhao</a>,
                                    <a href="https://scholar.google.com/citations?user=NSeLtc0AAAAJ&hl=en">Jiachen Liu</a>,
                                    Sheng Zhang,
                                    <a href="https://liy1shu.github.io/">Yishu Li</a>,
                                    Sili Chen,
                                    <a href="https://faculty.ist.psu.edu/suh972/">Sharon X Huang</a>,
                                    <a href="https://yongjinliu.github.io/">Yong-Jin Liu</a>,
                                    <strong>Hengkai Guo</strong>
                                    <br>
                                    <em>IROS</em>, 2024 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                                    <br>
                                    <a href="https://github.com/thuzhaowang/MonoPlane">code (Coming soon)</a>
                                    /
                                    <a href="https://arxiv.org/abs/2411.01226">arXiv</a>
                                    <p></p>
                                    <p>
                                        Leverage pretrained depth and normal models to facilitate zero-shot monocular plane reconstruction.
                                    </p>
                                </td>
                            </tr>
                            <tr>
                                <td style="padding:16px;width:30%;vertical-align:middle">
                                    <img style="width:100%" src="images/lazyloc.png" alt="dise">
                                </td>
                                <td style="padding:8px;width:70%;vertical-align:middle">
                                    <a href="https://arxiv.org/abs/2307.09981">
                                        <span class="papertitle">Lazy Visual Localization via Motion Averaging</span>
                                    </a>
                                    <br>
                                    <a href="https://siyandong.github.io/">Siyan Dong</a>*,
                                    <a href="http://b1ueber2y.me/">Shaohui Liu</a>*,
                                    <strong>Hengkai Guo</strong>,
                                    <a href="https://baoquanchen.info/">Baoquan Chen</a>,
                                    <a href="https://people.inf.ethz.ch/marc.pollefeys/">Marc Pollefeys</a>
                                    <br>
                                    <em>arXiv</em>, 2024
                                    <br>
                                    <a href="https://arxiv.org/abs/2307.09981">arXiv</a>
                                    <p></p>
                                    <p>
                                        Integrate motion averaging into visual localization to eliminate the need for pre-built 3D maps.
                                    </p>
                                </td>
                            </tr>
                            <tr>
                                <td style="padding:4px;width:30%;vertical-align:middle">
                                    <img style="width:100%" src="images/particlesfm.gif" alt="dise">
                                </td>
                                <td style="padding:8px;width:70%;vertical-align:middle">
                                    <a href="http://b1ueber2y.me/projects/ParticleSfM/index.html">
                                        <span class="papertitle">ParticleSfM: Exploiting Dense Point Trajectories for Localizing Moving Cameras</span>
                                    </a>
                                    <br>
                                    <a href="https://thuzhaowang.github.io/">Wang Zhao</a>,
                                    <a href="http://b1ueber2y.me/">Shaohui Liu</a>,
                                    <strong>Hengkai Guo</strong>,
                                    <a href="https://engineering.tamu.edu/cse/profiles/Wang-Wenping.html">Wenping Wang</a>,
                                    <a href="https://yongjinliu.github.io/">Yong-Jin Liu</a>
                                    <br>
                                    <em>ECCV</em>, 2022
                                    <br>
                                    <a href="http://b1ueber2y.me/projects/ParticleSfM/index.html">project page</a>
                                    /
                                    <a href="https://arxiv.org/abs/2207.09137">arXiv</a>
                                    /
                                    <a href="https://github.com/bytedance/particle-sfm">code</a>
                                    <p></p>
                                    <p>
                                        A video structure-from-motion system with dense point trajectories that generalizes well to in-the-wild sequences with dynamic motion.
                                    </p>
                                </td>
                            </tr>
                            <tr>
                                <td style="padding:4px;width:30%;vertical-align:middle">
                                    <img style="width:100%" src="images/idnsolver.gif" alt="dise">
                                </td>
                                <td style="padding:8px;width:70%;vertical-align:middle">
                                    <a href="http://b1ueber2y.me/projects/IDN-Solver/">
                                        <span class="papertitle">A Confidence-based Iterative Solver of Depths and Surface Normals for Deep Multi-view Stereo</span>
                                    </a>
                                    <br>
                                    <a href="https://thuzhaowang.github.io/">Wang Zhao</a>*,
                                    <a href="http://b1ueber2y.me/">Shaohui Liu</a>*,
                                    <a href="https://weiyithu.github.io/">Yi Wei</a>,
                                    <strong>Hengkai Guo</strong>,
                                    <a href="https://yongjinliu.github.io/">Yong-Jin Liu</a>
                                    <br>
                                    <em>ICCV</em>, 2021
                                    <br>
                                    <a href="http://b1ueber2y.me/projects/IDN-Solver/">project page</a>
                                    /
                                    <a href="https://arxiv.org/abs/2201.07609v1">arXiv</a>
                                    /
                                    <a href="https://github.com/thuzhaowang/idn-solver">code</a>
                                    <p></p>
                                    <p>
                                        A differentiable solver for multi-view stereo that iteratively solves for per-view depth and normal map based on the locally planar assumption.
                                    </p>
                                </td>
                            </tr>
                            <tr>
                                <td style="padding:16px;width:30%;vertical-align:middle">
                                    <img style="width:100%" src="images/ifm.png" alt="dise">
                                </td>
                                <td style="padding:8px;width:70%;vertical-align:middle">
                                    <a href="https://ieeexplore.ieee.org/document/9564031">
                                        <span class="papertitle">Iterative Feature Matching for Self-supervised Indoor Depth Estimation</span>
                                    </a>
                                    <br>
                                    <a href="https://weiyithu.github.io/">Yi Wei</a>,
                                    <strong>Hengkai Guo</strong>,
                                    <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>,
                                    <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>
                                    <br>
                                    <em>TCSVT</em>, 2021
                                    <br>
                                    <a href="https://ieeexplore.ieee.org/document/9564031">paper</a>
                                    <p></p>
                                    <p>
                                        A self-supervised depth estimation framework designed for low-texture indoor scenes, which operates without PoseNet by utilizing iterative feature matching.
                                    </p>
                                </td>
                            </tr>
                            <tr>
                                <td style="padding:4px;width:30%;vertical-align:middle">
                                    <img style="width:100%" src="images/gpo.png" alt="dise">
                                </td>
                                <td style="padding:8px;width:70%;vertical-align:middle">
                                    <a href="https://arxiv.org/abs/2004.12051">
                                        <span class="papertitle">GPO: Global Plane Optimization for Fast and Accurate Monocular SLAM Initialization</span>
                                    </a>
                                    <br>
                                    Sicong Du*,
                                    <strong>Hengkai Guo</strong>*,
                                    <a href="https://scholar.google.com/citations?user=hMYxUpQAAAAJ&hl=en">Yao Chen</a>,
                                    Yilun Lin,
                                    Xiangbing Meng,
                                    Linfu Wen,
                                    <a href="https://people.ucas.ac.cn/~wangfeiyue?language=en">Fei-Yue Wang</a>
                                    <br>
                                    <em>ICRA</em>, 2020
                                    <br>
                                    <a href="https://arxiv.org/abs/2004.12051">arXiv</a>
                                    <p></p>
                                    <p>
                                        A monocular initialization method for SLAM using multi-frame planar homographies.
                                    </p>
                                </td>
                            </tr>
                            <tr>
                                <td style="padding:4px;width:30%;vertical-align:middle">
                                    <img style="width:100%" src="images/geopretrain.png" alt="dise">
                                </td>
                                <td style="padding:8px;width:70%;vertical-align:middle">
                                    <a href="https://github.com/HKUST-Aerial-Robotics/GeometricPretraining">
                                        <span class="papertitle">Geometric Pretraining for Monocular Depth Estimation</span>
                                        <br>
                                        <a href="https://wang-kx.github.io/">Kaixuan Wang</a>,
                                        <a href="https://scholar.google.com/citations?user=hMYxUpQAAAAJ&hl=en">Yao Chen</a>,
                                        <strong>Hengkai Guo</strong>,
                                        Linfu Wen,
                                        <a href="https://uav.hkust.edu.hk/group/">Shaojie Shen</a>
                                        <br>
                                        <em>ICRA</em>, 2020
                                        <br>
                                        <a href="https://ieeexplore.ieee.org/document/9196847">paper</a>
                                        /
                                        <a href="https://github.com/HKUST-Aerial-Robotics/GeometricPretraining">code</a>
                                        <p></p>
                                        <p>
                                            Use self-supervised flow pretraining task to improve monocular depth estimation.
                                        </p>
                                </td>
                            </tr>
                            <tr>
                                <td style="padding:4px;width:30%;vertical-align:middle">
                                    <img style="width:100%" src="images/poseren.png" alt="dise">
                                </td>
                                <td style="padding:8px;width:70%;vertical-align:middle">
                                    <a href="https://arxiv.org/abs/1708.03416">
                                        <span class="papertitle">Pose Guided Structured Region Ensemble Network for Cascaded Hand Pose Estimation</span>
                                    </a>
                                    <br>
                                    <a href="https://www.xinghaochen.xyz/">Xinghao Chen</a>,
                                    <a href="http://web.ee.tsinghua.edu.cn/wangguijin/en/index.htm">Guijin Wang</a>,
                                    <strong>Hengkai Guo</strong>,
                                    Cairong Zhang
                                    <br>
                                    <em>Neurocomputing</em>, 2019
                                    <br>
                                    <a href="https://arxiv.org/abs/1708.03416">arXiv</a>
                                    /
                                    <a href="https://github.com/xinghaochen/Pose-REN">code</a>
                                    <p></p>
                                    <p>
                                        Enhance RGB-D-based 3D hand pose estimation by fusing features from initial pose guidance, estimated through a region ensemble network.
                                    </p>
                                </td>
                            </tr>
                            <tr>
                                <td style="padding:4px;width:30%;vertical-align:middle">
                                    <img style="width:100%" src="images/bihand.png" alt="dise">
                                </td>
                                <td style="padding:8px;width:70%;vertical-align:middle">
                                    <a href="https://arxiv.org/abs/1804.10160">
                                        <span class="papertitle">Two-Stream Binocular Network: Accurate Near Field Finger Detection Based On Binocular Images</span>
                                    </a>
                                    <br>
                                    <a href="https://weiyithu.github.io/">Yi Wei</a>,
                                    <a href="http://web.ee.tsinghua.edu.cn/wangguijin/en/index.htm">Guijin Wang</a>,
                                    Cairong Zhang,
                                    <strong>Hengkai Guo</strong>,
                                    <a href="https://www.xinghaochen.xyz/">Xinghao Chen</a>,
                                    <a href="https://www.ee.tsinghua.edu.cn/en/info/1067/1292.htm">Huazhong Yang</a>
                                    <br>
                                    <em>VCIP</em>, 2017 &nbsp <font color="red"><strong>(Best Student Paper Award)</strong></font>
                                    <br>
                                    <a href="https://arxiv.org/abs/1804.10160">arXiv</a>
                                    /
                                    <a href="https://sites.google.com/view/thuhand17">dataset</a>
                                    <p></p>
                                    <p>
                                        A two-stream model designed to regress the 3D positions of fingertips from binocular images.
                                    </p>
                                </td>
                            </tr>
                            <tr>
                                <td style="padding:8px;width:30%;vertical-align:middle">
                                    <img style="width:100%" src="images/ren.gif" alt="dise">
                                </td>
                                <td style="padding:8px;width:70%;vertical-align:middle">
                                    <a href="https://github.com/guohengkai/region-ensemble-network">
                                        <span class="papertitle">Region Ensemble Network: Improving Convolutional Network for Hand Pose Estimation</span>
                                    </a>
                                    <br>
                                    <strong>Hengkai Guo</strong>,
                                    <a href="http://web.ee.tsinghua.edu.cn/wangguijin/en/index.htm">Guijin Wang</a>,
                                    <a href="https://www.xinghaochen.xyz/">Xinghao Chen</a>,
                                    Cairong Zhang,
                                    <a href="https://www.ee.tsinghua.edu.cn/en/info/1068/1298.htm">Fei Qiao</a>,
                                    <a href="https://www.ee.tsinghua.edu.cn/en/info/1067/1292.htm">Huazhong Yang</a>
                                    <br>
                                    <em>ICIP</em>, 2017
                                    <br>
                                    <a href="https://arxiv.org/abs/1702.02447">arXiv</a>
                                    /
                                    <a href="https://github.com/guohengkai/region-ensemble-network">code</a>
                                    <p></p>
                                    <p>
                                        A state-of-the-art RGB-D-based 3D hand pose model that utilizes a convolutional network and regional feature ensemble.
                                    </p>
                                </td>
                            </tr>
                            <tr>
                                <td style="padding:4px;width:30%;vertical-align:middle">
                                    <img style="width:100%" src="images/mfarnn.png" alt="dise">
                                </td>
                                <td style="padding:8px;width:70%;vertical-align:middle">
                                    <a href="https://arxiv.org/abs/1708.03278">
                                        <span class="papertitle">Motion Feature Augmented Recurrent Neural Network for Skeleton-based Dynamic Hand Gesture Recognition</span>
                                    </a>
                                    <br>
                                    <a href="https://www.xinghaochen.xyz/">Xinghao Chen</a>,
                                    <strong>Hengkai Guo</strong>,
                                    <a href="http://web.ee.tsinghua.edu.cn/wangguijin/en/index.htm">Guijin Wang</a>,
                                    Cairong Zhang,
                                    <a href="https://www.ee.tsinghua.edu.cn/en/info/1064/1272.htm">Li Zhang</a>
                                    <br>
                                    <em>ICIP</em>, 2017
                                    <br>
                                    <a href="https://arxiv.org/abs/1708.03278">arXiv</a>
                                    <p></p>
                                    <p>
                                        Use a recurrent neural network with augmented motion features to estimate skeleton-based dynamic hand gesture.
                                    </p>
                                </td>
                            </tr>
                            <table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;">
                                <tbody>
                                    <tr>
                                        <td>
                                            <h2>Miscellanea</h2>
                                        </td>
                                    </tr>
                                </tbody>
                            </table>
                            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                                <tbody>
                                    <tr>
                                        <td align="center" style="padding:16px;width:20%;vertical-align:middle">
                                            <div class="colored-box" style="background-color: #fcb97d;">
                                                <h2>Competitions</h2>
                                            </div>
                                        </td>
                                        <td style="padding:8px;width:80%;vertical-align:middle">
                                            <a href="https://www.objects365.org/workshop2019.html">Detection In the Wild Challenge in CVPR 2019</a>: <font color="red"><strong>2nd for Object365 Full Track</strong></font>
                                            <br>
                                            <a href="https://openaccess.thecvf.com/ECCV2018_workshops/ECCV2018_W9">PoseTrack Challenge in ECCV 2018</a>: <font color="red"><strong>2nd for human pose tracking without extra datasets</strong></font>
                                        </td>
                                    </tr>
                                    <tr>
                                        <td align="center" style="padding:16px;width:20%;vertical-align:middle">
                                            <div class="colored-box" style="background-color: #aaba9e;">
                                                <h2>Honors and Awards</h2>
                                            </div>
                                        </td>
                                        <td style="padding:8px;width:80%;vertical-align:center">
                                            Intel Scholarship, 2016 <font color="red"><strong>(5 people in Tsinghua University)</strong></font>
                                            <br>
                                            Guanghua Scholarship, 2015
                                            <br>
                                            <a href="https://www.tsinghua.edu.cn/info/1173/18354.htm">Excellent Graduate in Tsinghua University, 2014</a>
                                            <br>
                                            <a href="https://www.tuef.tsinghua.edu.cn/info/1042/2323.htm">Nanxiang Jiang Scholarship, 2013</a>
                                            <font color="red"><strong>(Top scholarship, 19 people in Tsinghua University)</strong></font>
                                        </td>
                                    </tr>
                                </tbody>
                            </table>
                            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                                <tbody>
                                    <tr>
                                        <td style="padding:0px">
                                            <br>
                                            <p style="text-align:center;font-size:small;">
                                                &copy; Hengkai Guo | Last updated: Feb 2025</a>
                                            </p>
                                            <p style="text-align:center;font-size:small;">
                                                Design and source code from <a style="font-size:small;" href="https://jonbarron.info">Jon Barron's website</a>
                                            </p>
                                        </td>
                                    </tr>
                                </tbody>
                            </table>
                </td>
            </tr>
    </table>
</body>

</html>